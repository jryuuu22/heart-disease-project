{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvQcvBUhvWeBakR42TOAGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jryuuu22/basketball-eclipse-project/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lMnJEqHJbLzq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ft_pressure_final.py\n",
        "\n",
        "Free Throw Pressure Model:\n",
        "- Prepares features from NBA play_by_play.csv\n",
        "- Trains a RandomForest with Optuna-tuned hyperparameters\n",
        "- Saves model + preprocessing artifacts\n",
        "- Provides FTPressurePredictor class for single + batch predictions\n",
        "- Includes evaluation and interpretability (feature importances, OOB, shot-level explanations)\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from typing import List, Dict, Union, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    brier_score_loss,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_COLS: List[str] = [\n",
        "    \"season_FT_pct\",\n",
        "    \"overall_ft_pct\",\n",
        "    \"clutch_ft_pct\",\n",
        "    \"clutch_factor\",\n",
        "    \"career_attempts_so_far\",\n",
        "    \"period\",\n",
        "    \"seconds_remaining\",\n",
        "    \"is_clutch\",\n",
        "    \"close_game\",\n",
        "    \"late_game\",\n",
        "    \"pressure_score\",\n",
        "    \"point_differential\",\n",
        "]\n",
        "\n",
        "# Best RF parameters from your Optuna run (rf_best_params)\n",
        "BEST_RF_PARAMS = {\n",
        "    \"n_estimators\": 188,\n",
        "    \"max_depth\": 4,\n",
        "    \"min_samples_split\": 6,\n",
        "    \"min_samples_leaf\": 11,\n",
        "    \"max_features\": \"log2\",\n",
        "    \"random_state\": 42,\n",
        "    \"n_jobs\": -1,\n",
        "    \"oob_score\": True,  # enable OOB for interpretability\n",
        "}\n",
        "\n",
        "\n",
        "# -------------------"
      ],
      "metadata": {
        "id": "k9TyVHwqbOTq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_player_name(description: Any) -> Optional[str]:\n",
        "    \"\"\"Extract player name from FT description like 'MISS Jordan Free Throw 2 of 2'.\"\"\"\n",
        "    if pd.isna(description):\n",
        "        return None\n",
        "    match = re.search(r\"^(?:MISS\\s+)?([A-Za-z\\.\\s]+?)\\s+Free Throw\", str(description))\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "\n",
        "def _time_to_seconds(time_str: Any) -> Optional[int]:\n",
        "    \"\"\"Convert 'MM:SS' to total seconds.\"\"\"\n",
        "    if pd.isna(time_str):\n",
        "        return None\n",
        "    try:\n",
        "        mm, ss = str(time_str).split(\":\")\n",
        "        return int(mm) * 60 + int(ss)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _parse_score_diff(score_str: Any) -> Optional[int]:\n",
        "    \"\"\"Parse '71 - 85' â†’ |71 - 85|.\"\"\"\n",
        "    if pd.isna(score_str):\n",
        "        return None\n",
        "    try:\n",
        "        home, away = score_str.split(\" - \")\n",
        "        return abs(int(home) - int(away))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def prepare_ft_dataset_from_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Feature-engineer a modeling DataFrame from raw play_by_play data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model_df : DataFrame\n",
        "        Contains FEATURE_COLS + ['FT_made', 'game_id'].\n",
        "    \"\"\"\n",
        "    df = input_df.copy()\n",
        "\n",
        "    # 1) Keep only free throw events\n",
        "    keyword = \"Free Throw\"\n",
        "    df = df[\n",
        "        df[[\"homedescription\", \"visitordescription\"]]\n",
        "        .apply(lambda x: x.astype(str).str.contains(keyword, case=False, na=False))\n",
        "        .any(axis=1)\n",
        "    ]"
      ],
      "metadata": {
        "id": "BZV28ewpbd3u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_ft_dataset_from_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Feature-engineer a modeling DataFrame from raw play_by_play data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model_df : DataFrame\n",
        "        Contains FEATURE_COLS + ['FT_made', 'game_id'].\n",
        "    \"\"\"\n",
        "    df = input_df.copy()\n",
        "\n",
        "    # 1) Keep only free throw events\n",
        "    keyword = \"Free Throw\"\n",
        "    df = df[\n",
        "        df[[\"homedescription\", \"visitordescription\"]]\n",
        "        .apply(lambda x: x.astype(str).str.contains(keyword, case=False, na=False))\n",
        "        .any(axis=1)\n",
        "    ]\n",
        "\n",
        "    # 2) Drop many unused columns\n",
        "    df = df.drop(\n",
        "        columns=[\n",
        "            \"neutraldescription\",\n",
        "            \"video_available_flag\",\n",
        "            \"person1type\",\n",
        "            \"player1_team_nickname\",\n",
        "            \"player1_team_abbreviation\",\n",
        "            \"player1_id\",\n",
        "            \"player1_name\",\n",
        "            \"player1_team_city\",\n",
        "            \"player1_team_id\",\n",
        "            \"person2type\",\n",
        "            \"player2_team_nickname\",\n",
        "            \"player2_team_abbreviation\",\n",
        "            \"player2_id\",\n",
        "            \"player2_name\",\n",
        "            \"player2_team_city\",\n",
        "            \"player2_team_id\",\n",
        "            \"person3type\",\n",
        "            \"player3_team_nickname\",\n",
        "            \"player3_team_abbreviation\",\n",
        "            \"player3_id\",\n",
        "            \"player3_name\",\n",
        "            \"player3_team_city\",\n",
        "            \"player3_team_id\",\n",
        "        ],\n",
        "        errors=\"ignore\",\n",
        "    )\n",
        "\n",
        "    # 3) Merge home/visitor descriptions into a single FT_Event\n",
        "    nan_idx = df[df[\"homedescription\"].isna()].index\n",
        "    df.loc[nan_idx, \"homedescription\"] = df.loc[nan_idx, \"visitordescription\"]\n",
        "    df = df.drop(columns=[\"visitordescription\"], errors=\"ignore\")\n",
        "    df = df.rename(columns={\"homedescription\": \"FT_Event\"})\n",
        "\n",
        "    # 4) Target: made vs missed FT\n",
        "    missed_mask = df[\"FT_Event\"].str.contains(\"MISS\", case=False, na=False)\n",
        "    df[\"FT_made\"] = (~missed_mask).astype(int)\n",
        "\n",
        "    # 5) Fix score with game-wise forward fill\n",
        "    df = df.sort_values([\"game_id\", \"period\", \"eventnum\"])\n",
        "    df[\"score\"] = df.groupby(\"game_id\")[\"score\"].ffill()\n",
        "    df = df.dropna(subset=[\"score\"])\n",
        "\n",
        "    # 6) Parse score to home/away and margin\n",
        "    score_clean = df[\"score\"].str.replace(\" \", \"\")\n",
        "    df[\"home_score\"] = score_clean.str.split(\"-\").str[0].astype(int)\n",
        "    df[\"away_score\"] = score_clean.str.split(\"-\").str[1].astype(int)\n",
        "    df[\"scoremargin\"] = df[\"home_score\"] - df[\"away_score\"]\n",
        "\n",
        "    # 7) Player name\n",
        "    df[\"player_name\"] = df[\"FT_Event\"].apply(_extract_player_name)\n",
        "\n",
        "    # 8) Time and clutch flags\n",
        "    df[\"seconds_remaining\"] = df[\"pctimestring\"].apply(_time_to_seconds)\n",
        "    df[\"is_clutch\"] = (\n",
        "        (df[\"period\"] >= 4) & (df[\"seconds_remaining\"] <= 300)\n",
        "    ).astype(int)\n",
        "    df[\"point_differential\"] = df[\"score\"].apply(_parse_score_diff)\n",
        "    df[\"high_pressure\"] = (\n",
        "        (df[\"is_clutch\"] == 1) & (df[\"point_differential\"] <= 5)\n",
        "    ).astype(int)\n",
        "\n",
        "    # 9) Player-level aggregates\n",
        "    # Season FT% (within this dataset)\n",
        "    season_ft = (\n",
        "        df.groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"season_FT_pct\"})\n",
        "    )\n",
        "    df = df.merge(season_ft, on=\"player_name\", how=\"left\")\n",
        "\n",
        "    # Overall FT%, clutch/high-pressure attempts, clutch FT%\n",
        "    player_stats = (\n",
        "        df.groupby(\"player_name\")\n",
        "        .agg(\n",
        "            {\n",
        "                \"FT_made\": [\"sum\", \"count\", \"mean\"],\n",
        "                \"is_clutch\": \"sum\",\n",
        "                \"high_pressure\": \"sum\",\n",
        "            }\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    player_stats.columns = [\n",
        "        \"player_name\",\n",
        "        \"ft_made_total\",\n",
        "        \"ft_attempts\",\n",
        "        \"overall_ft_pct\",\n",
        "        \"clutch_attempts\",\n",
        "        \"high_pressure_attempts\",\n",
        "    ]\n",
        "\n",
        "    clutch_stats = (\n",
        "        df[df[\"is_clutch\"] == 1]\n",
        "        .groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"clutch_ft_pct\"})\n",
        "    )\n",
        "    high_pressure_stats = (\n",
        "        df[df[\"high_pressure\"] == 1]\n",
        "        .groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"high_pressure_ft_pct\"})\n",
        "    )\n",
        "\n",
        "    player_stats = player_stats.merge(clutch_stats, on=\"player_name\", how=\"left\")\n",
        "    player_stats = player_stats.merge(high_pressure_stats, on=\"player_name\", how=\"left\")\n",
        "    player_stats[\"clutch_factor\"] = (\n",
        "        player_stats[\"clutch_ft_pct\"] - player_stats[\"overall_ft_pct\"]\n",
        "    )\n",
        "\n",
        "    df = df.merge(\n",
        "        player_stats[[\"player_name\", \"overall_ft_pct\", \"clutch_ft_pct\", \"clutch_factor\"]],\n",
        "        on=\"player_name\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # 10) Context flags\n",
        "    df[\"close_game\"] = (df[\"point_differential\"] <= 3).astype(int)\n",
        "    df[\"late_game\"] = (df[\"period\"] >= 4).astype(int)\n",
        "    df[\"pressure_score\"] = (\n",
        "        df[\"is_clutch\"].astype(int)\n",
        "        + df[\"close_game\"].astype(int)\n",
        "        + (df[\"seconds_remaining\"] <= 120).astype(int)\n",
        "    )\n",
        "\n",
        "    # 11) Career attempts so far (within this dataset)\n",
        "    career_attempts = df.groupby(\"player_name\").size().to_dict()\n",
        "    df[\"career_attempts_so_far\"] = df[\"player_name\"].map(career_attempts)\n",
        "\n",
        "    # Final modeling frame\n",
        "    model_df = df[FEATURE_COLS + [\"FT_made\", \"game_id\"]].copy()\n",
        "    model_df = model_df.dropna(subset=[\"FT_made\", \"game_id\"])\n",
        "\n",
        "    return model_df"
      ],
      "metadata": {
        "id": "C4x87iCqbkQP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_model(\n",
        "    play_by_play_path: str,\n",
        "    nrows: Optional[int] = None,\n",
        "    test_size: float = 0.2,\n",
        "    model_path: str = \"final_rf.pkl\",\n",
        "    imputer_path: str = \"imputer.pkl\",\n",
        "    scaler_path: str = \"scaler.pkl\",\n",
        "    x_test_path: str = \"X_test_raw.csv\",\n",
        "    y_test_path: str = \"y_test.csv\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    End-to-end training from play_by_play CSV.\n",
        "\n",
        "    - Game-level train/test split (no leakage)\n",
        "    - Impute + scale features\n",
        "    - Train RandomForest with BEST_RF_PARAMS (OOB enabled)\n",
        "    - Save model, imputer, scaler, and raw test set (for later evaluation)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict\n",
        "        {'accuracy', 'pr_auc', 'brier', 'oob_score'}\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_csv(play_by_play_path, nrows=nrows)\n",
        "    model_df = prepare_ft_dataset_from_df(raw_df)\n",
        "\n",
        "    X = model_df[FEATURE_COLS].copy()\n",
        "    y = model_df[\"FT_made\"].astype(int).copy()\n",
        "    game_ids = model_df[\"game_id\"].unique()\n",
        "\n",
        "    # Game-level split\n",
        "    train_games, test_games = train_test_split(\n",
        "        game_ids, test_size=test_size, shuffle=False\n",
        "    )\n",
        "    train_mask = model_df[\"game_id\"].isin(train_games)\n",
        "    test_mask = model_df[\"game_id\"].isin(test_games)\n",
        "\n",
        "    X_train_raw = X[train_mask].copy()\n",
        "    X_test_raw = X[test_mask].copy()\n",
        "    y_train = y[train_mask].copy()\n",
        "    y_test = y[test_mask].copy()\n",
        "\n",
        "    # Impute + scale\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X_train_imp = pd.DataFrame(\n",
        "        imputer.fit_transform(X_train_raw),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_train_raw.index,\n",
        "    )\n",
        "    X_test_imp = pd.DataFrame(\n",
        "        imputer.transform(X_test_raw),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_test_raw.index,\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train_imp),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_train_imp.index,\n",
        "    )\n",
        "    X_test_scaled = pd.DataFrame(\n",
        "        scaler.transform(X_test_imp),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_test_imp.index,\n",
        "    )\n",
        "\n",
        "    # Model with OOB\n",
        "    model = RandomForestClassifier(**BEST_RF_PARAMS)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Test metrics\n",
        "    y_proba_test = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred_test)\n",
        "    pr_auc = average_precision_score(y_test, y_proba_test)\n",
        "    brier = brier_score_loss(y_test, y_proba_test)\n",
        "    oob = float(model.oob_score_)\n",
        "\n",
        "    # Save artifacts\n",
        "    X_test_raw.to_csv(x_test_path, index=False)\n",
        "    y_test.to_csv(y_test_path, index=False)\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "    with open(imputer_path, \"wb\") as f:\n",
        "        pickle.dump(imputer, f)\n",
        "    with open(scaler_path, \"wb\") as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"pr_auc\": float(pr_auc),\n",
        "        \"brier\": float(brier),\n",
        "        \"oob_score\": oob,\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "M04YDcqObmYg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionExplanation:\n",
        "    \"\"\"\n",
        "    Explanation object for a single prediction.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    prob_make : float\n",
        "    predicted_label : int\n",
        "    top_factors : List[Dict[str, Any]]\n",
        "        Each item: {'feature', 'value', 'contribution_pct}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        prob_make: float,\n",
        "        predicted_label: int,\n",
        "        top_factors: List[Dict[str, Any]],\n",
        "    ):\n",
        "        self.prob_make = prob_make\n",
        "        self.predicted_label = predicted_label\n",
        "        self.top_factors = top_factors\n",
        "\n",
        "\n",
        "class FTPressurePredictor:\n",
        "    \"\"\"\n",
        "    Wrapper around trained RandomForest + preprocessing.\n",
        "\n",
        "    Supports:\n",
        "    - Single and batch predictions\n",
        "    - Global feature importances\n",
        "    - Simple per-shot explanations that attribute contributions\n",
        "      using feature_importances_ and scaled feature values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"final_rf.pkl\",\n",
        "        imputer_path: str = \"imputer.pkl\",\n",
        "        scaler_path: str = \"scaler.pkl\",\n",
        "        feature_cols: Optional[List[str]] = None,\n",
        "    ):\n",
        "        self.feature_cols = feature_cols or FEATURE_COLS\n",
        "\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            self.model = pickle.load(f)\n",
        "        with open(imputer_path, \"rb\") as f:\n",
        "            self.imputer = pickle.load(f)\n",
        "        with open(scaler_path, \"rb\") as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "\n",
        "        if not hasattr(self.model, \"feature_importances_\"):\n",
        "            raise ValueError(\n",
        "                \"Loaded model does not expose feature_importances_. \"\n",
        "                \"Use a tree-based model such as RandomForest.\"\n",
        "            )\n",
        "\n",
        "        if len(self.model.feature_importances_) != len(self.feature_cols):\n",
        "            raise ValueError(\n",
        "                \"Feature length mismatch between model and feature_cols. \"\n",
        "                \"Check training vs. inference configuration.\"\n",
        "            )\n",
        "\n",
        "    # ---- internal helpers ----\n",
        "    def _to_dataframe(\n",
        "        self, X: Union[Dict[str, float], pd.Series, pd.DataFrame]\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Coerce input into a DataFrame with correct column order.\"\"\"\n",
        "        if isinstance(X, dict):\n",
        "            df = pd.DataFrame([X])\n",
        "        elif isinstance(X, pd.Series):\n",
        "            df = X.to_frame().T\n",
        "        elif isinstance(X, pd.DataFrame):\n",
        "            df = X.copy()\n",
        "        else:\n",
        "            raise TypeError(\"X must be dict, pandas Series, or pandas DataFrame.\")\n",
        "\n",
        "        missing = [c for c in self.feature_cols if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required feature(s): {missing}\")\n",
        "\n",
        "        df = df[self.feature_cols]\n",
        "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        return df\n",
        "\n",
        "    def _prepare_for_model(self, X_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply imputer + scaler to raw features.\"\"\"\n",
        "        X_imp = pd.DataFrame(\n",
        "            self.imputer.transform(X_raw),\n",
        "            columns=self.feature_cols,\n",
        "            index=X_raw.index,\n",
        "        )\n",
        "        X_scaled = pd.DataFrame(\n",
        "            self.scaler.transform(X_imp),\n",
        "            columns=self.feature_cols,\n",
        "            index=X_imp.index,\n",
        "        )\n",
        "        return X_scaled\n",
        "\n",
        "    # ---- public API ----\n",
        "    def predict_proba(\n",
        "        self, X: Union[Dict[str, float], pd.Series, pd.DataFrame]\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict P(FT make) for one or many rows of *raw* features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        probs : np.ndarray of shape (n_samples,)\n",
        "        \"\"\"\n",
        "        df_raw = self._to_dataframe(X)\n",
        "        X_scaled = self._prepare_for_model(df_raw)\n",
        "        probs = self.model.predict_proba(X_scaled)[:, 1]\n",
        "        return probs\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        X: Union[Dict[str, float], pd.Series, pd.DataFrame],\n",
        "        threshold: float = 0.5,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict binary outcome (1 = make, 0 = miss).\n",
        "        \"\"\"\n",
        "        probs = self.predict_proba(X)\n",
        "        return (probs >= threshold).astype(int)\n",
        "\n",
        "    def predict_single_with_explanation(\n",
        "        self,\n",
        "        X: Union[Dict[str, float], pd.Series],\n",
        "        threshold: float = 0.5,\n",
        "        top_k: int = 3,\n",
        "    ) -> PredictionExplanation:\n",
        "        \"\"\"\n",
        "        Single-shot prediction with human-friendly explanation.\n",
        "\n",
        "        Explanation idea:\n",
        "        - Take global feature_importances_ (how much the model relies on each feature).\n",
        "        - Multiply by the absolute value of this shot's scaled feature values.\n",
        "        - Normalize to get contribution percentages.\n",
        "        \"\"\"\n",
        "        df_raw = self._to_dataframe(X)\n",
        "        if len(df_raw) != 1:\n",
        "            raise ValueError(\"predict_single_with_explanation expects exactly one row.\")\n",
        "\n",
        "        X_scaled = self._prepare_for_model(df_raw)\n",
        "        prob_make = float(self.predict_proba(df_raw)[0])\n",
        "        predicted_label = int(prob_make >= threshold)\n",
        "\n",
        "        fi = np.array(self.model.feature_importances_, dtype=float)\n",
        "        fi = np.maximum(fi, 0.0)\n",
        "\n",
        "        row_vals = X_scaled.iloc[0].values.astype(float)\n",
        "        contrib_scores = np.abs(row_vals) * fi\n",
        "        total = contrib_scores.sum()\n",
        "        if total <= 0:\n",
        "            contrib_scores = fi\n",
        "            total = contrib_scores.sum() if contrib_scores.sum() > 0 else 1.0\n",
        "\n",
        "        contrib_pct = (contrib_scores / total) * 100.0\n",
        "\n",
        "        contrib_df = (\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"feature\": self.feature_cols,\n",
        "                    \"value\": row_vals,\n",
        "                    \"contribution_pct\": contrib_pct,\n",
        "                }\n",
        "            )\n",
        "            .sort_values(\"contribution_pct\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        top = contrib_df.head(top_k)\n",
        "        top_factors = [\n",
        "            {\n",
        "                \"feature\": r[\"feature\"],\n",
        "                \"value\": float(r[\"value\"]),\n",
        "                \"contribution_pct\": float(r[\"contribution_pct\"]),\n",
        "            }\n",
        "            for _, r in top.iterrows()\n",
        "        ]\n",
        "\n",
        "        return PredictionExplanation(\n",
        "            prob_make=prob_make,\n",
        "            predicted_label=predicted_label,\n",
        "            top_factors=top_factors,\n",
        "        )\n",
        "\n",
        "    def feature_importances(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Global feature importance ranking as a tidy DataFrame.\n",
        "        \"\"\"\n",
        "        fi = np.array(self.model.feature_importances_, dtype=float)\n",
        "        total = fi.sum() if fi.sum() > 0 else 1.0\n",
        "        pct = (fi / total) * 100.0\n",
        "        return (\n",
        "            pd.DataFrame(\n",
        "                {\"feature\": self.feature_cols, \"importance\": fi, \"importance_pct\": pct}\n",
        "            )\n",
        "            .sort_values(\"importance\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "    def get_oob_score(self) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Return the model's out-of-bag (OOB) score if available.\n",
        "        \"\"\"\n",
        "        return getattr(self.model, \"oob_score_\", None)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluation helper\n",
        "# ------------------------------\n",
        "def evaluate_model(\n",
        "    predictor: FTPressurePredictor,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: Union[pd.Series, np.ndarray],\n",
        "    threshold: float = 0.5,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate a trained predictor on a given test set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predictor : FTPressurePredictor\n",
        "    X_test : DataFrame with raw feature columns (FEATURE_COLS)\n",
        "    y_test : Series/array of 0/1 labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict\n",
        "        {'accuracy', 'pr_auc', 'brier', 'roc_auc'}\n",
        "    \"\"\"\n",
        "    if isinstance(y_test, pd.Series):\n",
        "        y = y_test.values\n",
        "    else:\n",
        "        y = y_test\n",
        "\n",
        "    probs = predictor.predict_proba(X_test)\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y, preds)\n",
        "    pr_auc = average_precision_score(y, probs)\n",
        "    brier = brier_score_loss(y, probs)\n",
        "    fpr, tpr, _ = roc_curve(y, probs)\n",
        "    roc_auc_val = auc(fpr, tpr)\n",
        "\n",
        "    cm = confusion_matrix(y, preds)\n",
        "\n",
        "    print(\"=== Test Set Performance ===\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"PR-AUC   : {pr_auc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {roc_auc_val:.4f}\")\n",
        "    print(f\"Brier    : {brier:.4f}\")\n",
        "    print(\"Confusion matrix [rows=true, cols=pred]:\")\n",
        "    print(cm)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"pr_auc\": float(pr_auc),\n",
        "        \"roc_auc\": float(roc_auc_val),\n",
        "        \"brier\": float(brier),\n",
        "    }"
      ],
      "metadata": {
        "id": "WR-XhqnVbvgw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 1. Write ft_pressure_final.py\n",
        "# ============================\n",
        "module_code = r\"\"\"\n",
        "import re\n",
        "from typing import List, Dict, Union, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    brier_score_loss,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "FEATURE_COLS: List[str] = [\n",
        "    \"season_FT_pct\",\n",
        "    \"overall_ft_pct\",\n",
        "    \"clutch_ft_pct\",\n",
        "    \"clutch_factor\",\n",
        "    \"career_attempts_so_far\",\n",
        "    \"period\",\n",
        "    \"seconds_remaining\",\n",
        "    \"is_clutch\",\n",
        "    \"close_game\",\n",
        "    \"late_game\",\n",
        "    \"pressure_score\",\n",
        "    \"point_differential\",\n",
        "]\n",
        "\n",
        "BEST_RF_PARAMS = {\n",
        "    \"n_estimators\": 188,\n",
        "    \"max_depth\": 4,\n",
        "    \"min_samples_split\": 6,\n",
        "    \"min_samples_leaf\": 11,\n",
        "    \"max_features\": \"log2\",\n",
        "    \"random_state\": 42,\n",
        "    \"n_jobs\": -1,\n",
        "    \"oob_score\": True,\n",
        "}\n",
        "\n",
        "\n",
        "def _extract_player_name(description: Any) -> Optional[str]:\n",
        "    if pd.isna(description):\n",
        "        return None\n",
        "    match = re.search(r\"^(?:MISS\\s+)?([A-Za-z\\.\\s]+?)\\s+Free Throw\", str(description))\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "\n",
        "def _time_to_seconds(time_str: Any) -> Optional[int]:\n",
        "    if pd.isna(time_str):\n",
        "        return None\n",
        "    try:\n",
        "        mm, ss = str(time_str).split(\":\")\n",
        "        return int(mm) * 60 + int(ss)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _parse_score_diff(score_str: Any) -> Optional[int]:\n",
        "    if pd.isna(score_str):\n",
        "        return None\n",
        "    try:\n",
        "        home, away = score_str.split(\" - \")\n",
        "        return abs(int(home) - int(away))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def prepare_ft_dataset_from_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = input_df.copy()\n",
        "\n",
        "    keyword = \"Free Throw\"\n",
        "    df = df[\n",
        "        df[[\"homedescription\", \"visitordescription\"]]\n",
        "        .apply(lambda x: x.astype(str).str.contains(keyword, case=False, na=False))\n",
        "        .any(axis=1)\n",
        "    ]\n",
        "\n",
        "    df = df.drop(\n",
        "        columns=[\n",
        "            \"neutraldescription\",\n",
        "            \"video_available_flag\",\n",
        "            \"person1type\",\n",
        "            \"player1_team_nickname\",\n",
        "            \"player1_team_abbreviation\",\n",
        "            \"player1_id\",\n",
        "            \"player1_name\",\n",
        "            \"player1_team_city\",\n",
        "            \"player1_team_id\",\n",
        "            \"person2type\",\n",
        "            \"player2_team_nickname\",\n",
        "            \"player2_team_abbreviation\",\n",
        "            \"player2_id\",\n",
        "            \"player2_name\",\n",
        "            \"player2_team_city\",\n",
        "            \"player2_team_id\",\n",
        "            \"person3type\",\n",
        "            \"player3_team_nickname\",\n",
        "            \"player3_team_abbreviation\",\n",
        "            \"player3_id\",\n",
        "            \"player3_name\",\n",
        "            \"player3_team_city\",\n",
        "            \"player3_team_id\",\n",
        "        ],\n",
        "        errors=\"ignore\",\n",
        "    )\n",
        "\n",
        "    nan_idx = df[df[\"homedescription\"].isna()].index\n",
        "    df.loc[nan_idx, \"homedescription\"] = df.loc[nan_idx, \"visitordescription\"]\n",
        "    df = df.drop(columns=[\"visitordescription\"], errors=\"ignore\")\n",
        "    df = df.rename(columns={\"homedescription\": \"FT_Event\"})\n",
        "\n",
        "    missed_mask = df[\"FT_Event\"].str.contains(\"MISS\", case=False, na=False)\n",
        "    df[\"FT_made\"] = (~missed_mask).astype(int)\n",
        "\n",
        "    df = df.sort_values([\"game_id\", \"period\", \"eventnum\"])\n",
        "    df[\"score\"] = df.groupby(\"game_id\")[\"score\"].ffill()\n",
        "    df = df.dropna(subset=[\"score\"])\n",
        "\n",
        "    score_clean = df[\"score\"].str.replace(\" \", \"\")\n",
        "    df[\"home_score\"] = score_clean.str.split(\"-\").str[0].astype(int)\n",
        "    df[\"away_score\"] = score_clean.str.split(\"-\").str[1].astype(int)\n",
        "    df[\"scoremargin\"] = df[\"home_score\"] - df[\"away_score\"]\n",
        "\n",
        "    df[\"player_name\"] = df[\"FT_Event\"].apply(_extract_player_name)\n",
        "    df[\"seconds_remaining\"] = df[\"pctimestring\"].apply(_time_to_seconds)\n",
        "    df[\"is_clutch\"] = (\n",
        "        (df[\"period\"] >= 4) & (df[\"seconds_remaining\"] <= 300)\n",
        "    ).astype(int)\n",
        "    df[\"point_differential\"] = df[\"score\"].apply(_parse_score_diff)\n",
        "    df[\"high_pressure\"] = (\n",
        "        (df[\"is_clutch\"] == 1) & (df[\"point_differential\"] <= 5)\n",
        "    ).astype(int)\n",
        "\n",
        "    season_ft = (\n",
        "        df.groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"season_FT_pct\"})\n",
        "    )\n",
        "    df = df.merge(season_ft, on=\"player_name\", how=\"left\")\n",
        "\n",
        "    player_stats = (\n",
        "        df.groupby(\"player_name\")\n",
        "        .agg(\n",
        "            {\n",
        "                \"FT_made\": [\"sum\", \"count\", \"mean\"],\n",
        "                \"is_clutch\": \"sum\",\n",
        "                \"high_pressure\": \"sum\",\n",
        "            }\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    player_stats.columns = [\n",
        "        \"player_name\",\n",
        "        \"ft_made_total\",\n",
        "        \"ft_attempts\",\n",
        "        \"overall_ft_pct\",\n",
        "        \"clutch_attempts\",\n",
        "        \"high_pressure_attempts\",\n",
        "    ]\n",
        "\n",
        "    clutch_stats = (\n",
        "        df[df[\"is_clutch\"] == 1]\n",
        "        .groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"clutch_ft_pct\"})\n",
        "    )\n",
        "    high_pressure_stats = (\n",
        "        df[df[\"high_pressure\"] == 1]\n",
        "        .groupby(\"player_name\")[\"FT_made\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"FT_made\": \"high_pressure_ft_pct\"})\n",
        "    )\n",
        "\n",
        "    player_stats = player_stats.merge(clutch_stats, on=\"player_name\", how=\"left\")\n",
        "    player_stats = player_stats.merge(high_pressure_stats, on=\"player_name\", how=\"left\")\n",
        "    player_stats[\"clutch_factor\"] = (\n",
        "        player_stats[\"clutch_ft_pct\"] - player_stats[\"overall_ft_pct\"]\n",
        "    )\n",
        "\n",
        "    df = df.merge(\n",
        "        player_stats[[\"player_name\", \"overall_ft_pct\", \"clutch_ft_pct\", \"clutch_factor\"]],\n",
        "        on=\"player_name\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    df[\"close_game\"] = (df[\"point_differential\"] <= 3).astype(int)\n",
        "    df[\"late_game\"] = (df[\"period\"] >= 4).astype(int)\n",
        "    df[\"pressure_score\"] = (\n",
        "        df[\"is_clutch\"].astype(int)\n",
        "        + df[\"close_game\"].astype(int)\n",
        "        + (df[\"seconds_remaining\"] <= 120).astype(int)\n",
        "    )\n",
        "\n",
        "    career_attempts = df.groupby(\"player_name\").size().to_dict()\n",
        "    df[\"career_attempts_so_far\"] = df[\"player_name\"].map(career_attempts)\n",
        "\n",
        "    model_df = df[FEATURE_COLS + [\"FT_made\", \"game_id\"]].copy()\n",
        "    model_df = model_df.dropna(subset=[\"FT_made\", \"game_id\"])\n",
        "\n",
        "    return model_df\n",
        "\n",
        "\n",
        "def train_and_save_model(\n",
        "    play_by_play_path: str,\n",
        "    nrows: Optional[int] = None,\n",
        "    test_size: float = 0.2,\n",
        "    model_path: str = \"final_rf.pkl\",\n",
        "    imputer_path: str = \"imputer.pkl\",\n",
        "    scaler_path: str = \"scaler.pkl\",\n",
        "    x_test_path: str = \"X_test_raw.csv\",\n",
        "    y_test_path: str = \"y_test.csv\",\n",
        ") -> Dict[str, float]:\n",
        "    raw_df = pd.read_csv(play_by_play_path, nrows=nrows)\n",
        "    model_df = prepare_ft_dataset_from_df(raw_df)\n",
        "\n",
        "    X = model_df[FEATURE_COLS].copy()\n",
        "    y = model_df[\"FT_made\"].astype(int).copy()\n",
        "    game_ids = model_df[\"game_id\"].unique()\n",
        "\n",
        "    train_games, test_games = train_test_split(\n",
        "        game_ids, test_size=test_size, shuffle=False\n",
        "    )\n",
        "    train_mask = model_df[\"game_id\"].isin(train_games)\n",
        "    test_mask = model_df[\"game_id\"].isin(test_games)\n",
        "\n",
        "    X_train_raw = X[train_mask].copy()\n",
        "    X_test_raw = X[test_mask].copy()\n",
        "    y_train = y[train_mask].copy()\n",
        "    y_test = y[test_mask].copy()\n",
        "\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X_train_imp = pd.DataFrame(\n",
        "        imputer.fit_transform(X_train_raw),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_train_raw.index,\n",
        "    )\n",
        "    X_test_imp = pd.DataFrame(\n",
        "        imputer.transform(X_test_raw),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_test_raw.index,\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train_imp),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_train_imp.index,\n",
        "    )\n",
        "    X_test_scaled = pd.DataFrame(\n",
        "        scaler.transform(X_test_imp),\n",
        "        columns=FEATURE_COLS,\n",
        "        index=X_test_imp.index,\n",
        "    )\n",
        "\n",
        "    model = RandomForestClassifier(**BEST_RF_PARAMS)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_proba_test = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred_test)\n",
        "    pr_auc = average_precision_score(y_test, y_proba_test)\n",
        "    brier = brier_score_loss(y_test, y_proba_test)\n",
        "    oob = float(model.oob_score_)\n",
        "\n",
        "    X_test_raw.to_csv(x_test_path, index=False)\n",
        "    y_test.to_csv(y_test_path, index=False)\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "    with open(imputer_path, \"wb\") as f:\n",
        "        pickle.dump(imputer, f)\n",
        "    with open(scaler_path, \"wb\") as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"pr_auc\": float(pr_auc),\n",
        "        \"brier\": float(brier),\n",
        "        \"oob_score\": oob,\n",
        "    }\n",
        "\n",
        "\n",
        "class PredictionExplanation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        prob_make: float,\n",
        "        predicted_label: int,\n",
        "        top_factors: List[Dict[str, Any]],\n",
        "    ):\n",
        "        self.prob_make = prob_make\n",
        "        self.predicted_label = predicted_label\n",
        "        self.top_factors = top_factors\n",
        "\n",
        "\n",
        "class FTPressurePredictor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"final_rf.pkl\",\n",
        "        imputer_path: str = \"imputer.pkl\",\n",
        "        scaler_path: str = \"scaler.pkl\",\n",
        "        feature_cols: Optional[List[str]] = None,\n",
        "    ):\n",
        "        self.feature_cols = feature_cols or FEATURE_COLS\n",
        "\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            self.model = pickle.load(f)\n",
        "        with open(imputer_path, \"rb\") as f:\n",
        "            self.imputer = pickle.load(f)\n",
        "        with open(scaler_path, \"rb\") as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "\n",
        "        if not hasattr(self.model, \"feature_importances_\"):\n",
        "            raise ValueError(\n",
        "                \"Loaded model does not expose feature_importances_. \"\n",
        "                \"Use a tree-based model such as RandomForest.\"\n",
        "            )\n",
        "\n",
        "        if len(self.model.feature_importances_) != len(self.feature_cols):\n",
        "            raise ValueError(\n",
        "                \"Feature length mismatch between model and feature_cols.\"\n",
        "            )\n",
        "\n",
        "    def _to_dataframe(\n",
        "        self, X: Union[Dict[str, float], pd.Series, pd.DataFrame]\n",
        "    ) -> pd.DataFrame:\n",
        "        if isinstance(X, dict):\n",
        "            df = pd.DataFrame([X])\n",
        "        elif isinstance(X, pd.Series):\n",
        "            df = X.to_frame().T\n",
        "        elif isinstance(X, pd.DataFrame):\n",
        "            df = X.copy()\n",
        "        else:\n",
        "            raise TypeError(\"X must be dict, pandas Series, or pandas DataFrame.\")\n",
        "\n",
        "        missing = [c for c in self.feature_cols if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required feature(s): {missing}\")\n",
        "\n",
        "        df = df[self.feature_cols]\n",
        "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        return df\n",
        "\n",
        "    def _prepare_for_model(self, X_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "        X_imp = pd.DataFrame(\n",
        "            self.imputer.transform(X_raw),\n",
        "            columns=self.feature_cols,\n",
        "            index=X_raw.index,\n",
        "        )\n",
        "        X_scaled = pd.DataFrame(\n",
        "            self.scaler.transform(X_imp),\n",
        "            columns=self.feature_cols,\n",
        "            index=X_imp.index,\n",
        "        )\n",
        "        return X_scaled\n",
        "\n",
        "    def predict_proba(\n",
        "        self, X: Union[Dict[str, float], pd.Series, pd.DataFrame]\n",
        "    ) -> np.ndarray:\n",
        "        df_raw = self._to_dataframe(X)\n",
        "        X_scaled = self._prepare_for_model(df_raw)\n",
        "        probs = self.model.predict_proba(X_scaled)[:, 1]\n",
        "        return probs\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        X: Union[Dict[str, float], pd.Series, pd.DataFrame],\n",
        "        threshold: float = 0.5,\n",
        "    ) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        return (probs >= threshold).astype(int)\n",
        "\n",
        "    def predict_single_with_explanation(\n",
        "        self,\n",
        "        X: Union[Dict[str, float], pd.Series],\n",
        "        threshold: float = 0.5,\n",
        "        top_k: int = 3,\n",
        "    ) -> PredictionExplanation:\n",
        "        df_raw = self._to_dataframe(X)\n",
        "        if len(df_raw) != 1:\n",
        "            raise ValueError(\"predict_single_with_explanation expects exactly one row.\")\n",
        "\n",
        "        X_scaled = self._prepare_for_model(df_raw)\n",
        "        prob_make = float(self.predict_proba(df_raw)[0])\n",
        "        predicted_label = int(prob_make >= threshold)\n",
        "\n",
        "        fi = np.array(self.model.feature_importances_, dtype=float)\n",
        "        fi = np.maximum(fi, 0.0)\n",
        "\n",
        "        row_vals = X_scaled.iloc[0].values.astype(float)\n",
        "        contrib_scores = np.abs(row_vals) * fi\n",
        "        total = contrib_scores.sum()\n",
        "        if total <= 0:\n",
        "            contrib_scores = fi\n",
        "            total = contrib_scores.sum() if contrib_scores.sum() > 0 else 1.0\n",
        "\n",
        "        contrib_pct = (contrib_scores / total) * 100.0\n",
        "\n",
        "        contrib_df = (\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"feature\": self.feature_cols,\n",
        "                    \"value\": row_vals,\n",
        "                    \"contribution_pct\": contrib_pct,\n",
        "                }\n",
        "            )\n",
        "            .sort_values(\"contribution_pct\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        top = contrib_df.head(top_k)\n",
        "        top_factors = [\n",
        "            {\n",
        "                \"feature\": r[\"feature\"],\n",
        "                \"value\": float(r[\"value\"]),\n",
        "                \"contribution_pct\": float(r[\"contribution_pct\"]),\n",
        "            }\n",
        "            for _, r in top.iterrows()\n",
        "        ]\n",
        "\n",
        "        return PredictionExplanation(\n",
        "            prob_make=prob_make,\n",
        "            predicted_label=predicted_label,\n",
        "            top_factors=top_factors,\n",
        "        )\n",
        "\n",
        "    def feature_importances(self) -> pd.DataFrame:\n",
        "        fi = np.array(self.model.feature_importances_, dtype=float)\n",
        "        total = fi.sum() if fi.sum() > 0 else 1.0\n",
        "        pct = (fi / total) * 100.0\n",
        "        return (\n",
        "            pd.DataFrame(\n",
        "                {\"feature\": self.feature_cols, \"importance\": fi, \"importance_pct\": pct}\n",
        "            )\n",
        "            .sort_values(\"importance\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "    def get_oob_score(self) -> Optional[float]:\n",
        "        return getattr(self.model, \"oob_score_\", None)\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    predictor: FTPressurePredictor,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: Union[pd.Series, np.ndarray],\n",
        "    threshold: float = 0.5,\n",
        ") -> Dict[str, float]:\n",
        "    if isinstance(y_test, pd.Series):\n",
        "        y = y_test.values\n",
        "    else:\n",
        "        y = y_test\n",
        "\n",
        "    probs = predictor.predict_proba(X_test)\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y, preds)\n",
        "    pr_auc = average_precision_score(y, probs)\n",
        "    brier = brier_score_loss(y, probs)\n",
        "    fpr, tpr, _ = roc_curve(y, probs)\n",
        "    roc_auc_val = auc(fpr, tpr)\n",
        "    cm = confusion_matrix(y, preds)\n",
        "\n",
        "    print(\"=== Test Set Performance ===\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"PR-AUC   : {pr_auc:.4f}\")\n",
        "    print(f\"ROC-AUC  : {roc_auc_val:.4f}\")\n",
        "    print(f\"Brier    : {brier:.4f}\")\n",
        "    print(\"Confusion matrix [rows=true, cols=pred]:\")\n",
        "    print(cm)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"pr_auc\": float(pr_auc),\n",
        "        \"roc_auc\": float(roc_auc_val),\n",
        "        \"brier\": float(brier),\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "with open(\"ft_pressure_final.py\", \"w\") as f:\n",
        "    f.write(module_code)\n",
        "\n",
        "import os\n",
        "print(\"Files in cwd:\", os.listdir())\n",
        "\n",
        "# ============================\n",
        "# 2. Import and use the module\n",
        "# ============================\n",
        "import importlib\n",
        "import ft_pressure_final\n",
        "importlib.reload(ft_pressure_final)\n",
        "\n",
        "from ft_pressure_final import (\n",
        "    train_and_save_model,\n",
        "    FTPressurePredictor,\n",
        "    evaluate_model,\n",
        ")\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "# Download dataset\n",
        "basketball_path = kagglehub.dataset_download(\"wyattowalsh/basketball\")\n",
        "csv_path = f\"{basketball_path}/csv\"\n",
        "pbp_path = f\"{csv_path}/play_by_play.csv\"\n",
        "print(\"Using:\", pbp_path)\n",
        "\n",
        "# Train and save model\n",
        "metrics = train_and_save_model(pbp_path, nrows=100000)\n",
        "print(\"Train metrics:\", metrics)\n",
        "\n",
        "# Load predictor\n",
        "predictor = FTPressurePredictor()\n",
        "\n",
        "# Evaluate on saved test set\n",
        "X_test = pd.read_csv(\"X_test_raw.csv\")\n",
        "y_test = pd.read_csv(\"y_test.csv\").iloc[:, 0]\n",
        "eval_metrics = evaluate_model(predictor, X_test, y_test)\n",
        "print(\"Eval metrics:\", eval_metrics)\n",
        "\n",
        "# Single-shot example with explanation\n",
        "example_shot = {\n",
        "    \"season_FT_pct\": 0.80,\n",
        "    \"overall_ft_pct\": 0.78,\n",
        "    \"clutch_ft_pct\": 0.85,\n",
        "    \"clutch_factor\": 0.07,\n",
        "    \"career_attempts_so_far\": 100,\n",
        "    \"period\": 4,\n",
        "    \"seconds_remaining\": 45,\n",
        "    \"is_clutch\": 1,\n",
        "    \"close_game\": 1,\n",
        "    \"late_game\": 1,\n",
        "    \"pressure_score\": 3,\n",
        "    \"point_differential\": 2,\n",
        "}\n",
        "exp = predictor.predict_single_with_explanation(example_shot)\n",
        "print(\"P(make):\", exp.prob_make)\n",
        "print(\"Label:\", exp.predicted_label)\n",
        "print(\"Top factors:\", exp.top_factors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZT_a-jXdtVM",
        "outputId": "d080e0cd-2166-4781-edce-e9c65f180b23"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in cwd: ['.config', 'final_rf.pkl', 'imputer.pkl', '__pycache__', 'scaler.pkl', 'ft_pressure_final.py', 'y_test.csv', 'X_test_raw.csv', 'sample_data']\n",
            "Using Colab cache for faster access to the 'basketball' dataset.\n",
            "Using: /kaggle/input/basketball/csv/play_by_play.csv\n",
            "Train metrics: {'accuracy': 0.7559306569343066, 'pr_auc': 0.8320305547620233, 'brier': 0.17782058094015893, 'oob_score': 0.735840756625976}\n",
            "=== Test Set Performance ===\n",
            "Accuracy : 0.7559\n",
            "PR-AUC   : 0.8320\n",
            "ROC-AUC  : 0.6375\n",
            "Brier    : 0.1778\n",
            "Confusion matrix [rows=true, cols=pred]:\n",
            "[[  31  515]\n",
            " [  20 1626]]\n",
            "Eval metrics: {'accuracy': 0.7559306569343066, 'pr_auc': 0.8320305547620233, 'roc_auc': 0.6374816961086707, 'brier': 0.17782058094015893}\n",
            "P(make): 0.8042499617828059\n",
            "Label: 1\n",
            "Top factors: [{'feature': 'season_FT_pct', 'value': 0.5829577750926249, 'contribution_pct': 32.51531387481439}, {'feature': 'overall_ft_pct', 'value': 0.3889053128440795, 'contribution_pct': 23.053519200567806}, {'feature': 'clutch_ft_pct', 'value': 0.6687318528550419, 'contribution_pct': 10.618390741707678}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ft_pressure_final import FTPressurePredictor\n",
        "\n",
        "predictor = FTPressurePredictor()\n",
        "\n",
        "# Single shot\n",
        "shot = {\n",
        "    \"season_FT_pct\": 0.80,\n",
        "    \"overall_ft_pct\": 0.78,\n",
        "    \"clutch_ft_pct\": 0.85,\n",
        "    \"clutch_factor\": 0.07,\n",
        "    \"career_attempts_so_far\": 100,\n",
        "    \"period\": 4,\n",
        "    \"seconds_remaining\": 45,\n",
        "    \"is_clutch\": 1,\n",
        "    \"close_game\": 1,\n",
        "    \"late_game\": 1,\n",
        "    \"pressure_score\": 3,\n",
        "    \"point_differential\": 2,\n",
        "}\n",
        "explanation = predictor.predict_single_with_explanation(shot)\n",
        "print(explanation.prob_make, explanation.predicted_label, explanation.top_factors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4_Eqzd2b3QA",
        "outputId": "2804804a-7721-43ae-de64-a3bc702771ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8042499617828059 1 [{'feature': 'season_FT_pct', 'value': 0.5829577750926249, 'contribution_pct': 32.51531387481439}, {'feature': 'overall_ft_pct', 'value': 0.3889053128440795, 'contribution_pct': 23.053519200567806}, {'feature': 'clutch_ft_pct', 'value': 0.6687318528550419, 'contribution_pct': 10.618390741707678}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Le-xBVKIb75K"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}